# ğŸ—ï¸ Airflow ETL Pipeline â€“ Olympic Athlete Data
This project implements an **Apache Airflow** ETL pipeline using **Docker**, designed to extract, transform, and load Olympic athlete data from CSV files using `pandas`.

---

## ğŸ“ Project Structure
airflow-etl-project/
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ etlpandas.py # Main ETL DAG using pandas
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ athlete_events.csv # Raw input data
â”‚ â””â”€â”€ athlete_cleaned.csv # Cleaned output data
â”œâ”€â”€ config/
â”‚ â””â”€â”€ airflow.cfg # Optional custom Airflow config
â”œâ”€â”€ .env # Environment variables (credentials/secrets)
â”œâ”€â”€ docker-compose.yaml # Dockerized Airflow stack
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/airflow-etl-project.git
cd airflow-etl-project
```

2. Setup Environment
Copy .env.example to .env (if used for secrets or configs):
```bash
cp .env.example .env
```
3. Start Airflow (via Docker)
```bash
docker-compose up airflow-init  # Initializes Airflow DB and user
docker-compose up               # Starts all services
```
Visit the Airflow web UI at:

ğŸ‘‰ http://localhost:8080
Login: airflow / airflow
